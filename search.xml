<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[比特币价格预测]]></title>
    <url>%2F999954af.html</url>
    <content type="text"><![CDATA[摘要分析2012年1月1日至2018年10月31日的比特币价格数据，并采用时间序列方法，构建自回归华东平均模型（ARMA模型），预测未来8个月比特币的价格走势。预测结果表明比特币将在8个月内降到4000美金左右，于实际比特币价格趋势吻合。 项目背景 项目来源：kaggle平台：https://www.kaggle.com/mczielinski/bitcoin-historical-data 比特币是运行时间最长，最着名的加密货币，由匿名中本聪于2009年首次作为开源发布。比特币作为数字交换的分散媒介，在公共分布式账本（区块链）中验证和记录交易，无需可信赖的记录保管机构或中央中介。事务块包含先前事务块的SHA-256加密哈希，因此被“链接”在一起，用作所有已发生事务的不可变记录。与市场上的任何货币/商品一样，比特币交易和金融工具很快跟随公众采用比特币并继续增长。这里包括以1分钟为间隔的历史比特币市场数据，用于交易发生的特定比特币交易所。 来源于kaggle项目描述算法选择（场景分析）比特币的价格是连续型的数值，可以采用回归分析和时间序列。比较一下两种算法的差异： 回归分析 回归分研究的是应变量与自变量的相关性，然后通过通过自变量新的观察值来预测应变量。 回归分析专注的是多变量与目标结果之间的分析，往往与时间无关。 时间序列 时间序列研究的是应变量与时间的相关性。 时间序列关注时间变化，分析目标变量的趋势、周期、时间和不稳定因素等特征。 时间序列算法中有些经典模型，包含 AR、MA、ARMA、ARIMA。 AR模型 AR英文全称autogressive，中文全称自回归模型。 算法思想：通过过去若干时刻的点通过线性组合，加上白噪声预测未来时刻的点。 AR(p)模型：p：自回归项数；采用过去时刻的p个点进行处理。 MA模型 MA英文全称Moving Average,中文全称 滑动平均模型 算法思想：通过历史白噪声进行线性组合影响当前时刻点，描述自回归部分的误差累计。 MA(q)模型：q：滑动平均项数。 ARMA模型 ARMA英文全称Autogressive Moving Average，中文全称自回归移动平均模型。 ARMA(p,q)模型：p与q两个阶数 ARIMA模型 ARMA英文全称Autoregressive Integrated Moving Average model ， 中文全称差分整合移动平均自回归模型。 ARIMA比ARMA多了差分的过程，作用是对不平稳数据进行差分平稳，在差分平稳后再建模。 ARIMA(p,q,d)：d:差分阶数。 ARMA的API1from statsmodels.tsa.arima_model import ARMA ARMA的参数 ARMA(endog,order,exog=None) endog：待分析的变量（data） order：代表p,q的值 exog ：外生变量，在经济机制内受外部因素影响ARMA的判断准则 AIC准则，又称赤道准则，是衡量统计模型拟合好坏的一个标准，数值越小代表模型拟合得越好。 数据预处理数据一共包含八个字段，具体含义如下： 字段 含义 Timestamp 日期 Open 开盘价 High 最高价 Low 最低价 Close 收盘价 Volume_(BTC) 比特币成交量 Volume_(Currency) 成交金额 Weighted_Price 比特币平均价格 12345678#加载数据df=pd.read_csv('./bitcoin_2012-01-01_to_2018-10-31.csv')#把时间作为df的索引df.Timestamp=pd.to_datetime(df.Timestamp)df.index=df.Timestamp#df.set_index(df.Timestamp)#查看数据集print(df.info()) &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; DatetimeIndex: 2497 entries, 2011-12-31 to 2018-10-31 Data columns (total 8 columns): Timestamp 2497 non-null datetime64[ns] Open 2494 non-null float64 High 2494 non-null float64 Low 2494 non-null float64 Close 2494 non-null float64 Volume_(BTC) 2494 non-null float64 Volume_(Currency) 2494 non-null float64 Weighted_Price 2494 non-null float64 dtypes: datetime64[ns](1), float64(7) memory usage: 175.6 KB None 发现Timestamp中有3天没有价格记录，用dropna()删除这3天的记录。 数据探索12#数据探索print(df.head()) Timestamp Open High Low Close Volume_(BTC) \ Timestamp 2011-12-31 2011-12-31 4.465000 4.482500 4.465000 4.482500 23.829470 2012-01-01 2012-01-01 4.806667 4.806667 4.806667 4.806667 7.200667 2012-01-02 2012-01-02 5.000000 5.000000 5.000000 5.000000 19.048000 2012-01-03 2012-01-03 5.252500 5.252500 5.252500 5.252500 11.004660 2012-01-04 2012-01-04 5.200000 5.223333 5.200000 5.223333 11.914807 Volume_(Currency) Weighted_Price Timestamp 2011-12-31 106.330084 4.471603 2012-01-01 35.259720 4.806667 2012-01-02 95.240000 5.000000 2012-01-03 58.100651 5.252500 2012-01-04 63.119578 5.208159 数据可视化查看比特币的历史走势，按照不同时间维度将数据压缩，并做可视化呈现。Weighted_Price是目标变量，对应比特币的每天价格；压缩后对应每月、每季度、每年的比特币平均价格。 1234#按照月份、季度与年份来进行统计df_month=df.resample('M').mean()df_Q=df.resample('Q-DEC').mean()df_year=df.resample('A-DEC').mean() 123456789101112131415161718#按照天，月，季度，年来显示比特币的走势fig=plt.figure(figsize=[15,7])#正常显示中文标签plt.rcParams['font.sans-serif']=['SimHei']plt.suptitle('比特币金额（美金）',fontsize=20)plt.subplot(221)plt.plot(df.Weighted_Price,'-',label='按天')plt.legend()plt.subplot(222)plt.plot(df_month.Weighted_Price,'-',label='按月')plt.legend()plt.subplot(223)plt.plot(df_Q.Weighted_Price,'-',label='按季度')plt.legend()plt.subplot(224)plt.plot(df_year.Weighted_Price,'-',label='按年')plt.legend()plt.show() 构建模型设置p,q参数：起初并不知道p,q取何值时模型预测效果更好。因此将其设定为一个区间范围，通过循环计算不同参数的AIC值，选择AIC值最优的参数作为最优模型。12345678910111213141516171819202122232425#设置参数范围ps=range(0,4)qs=range(0,4)parameters=product(ps,qs)parameters_list=list(parameters)#寻找最优ARMA模型参数，即best_aic最小results=[]best_aic=float("inf") #正无穷for param in parameters_list: try: model=ARMA(df_month.Weighted_Price,order=(param[0],param[1])).fit() except ValueError: print("参数错误：",param) continue aic=model.aic if aic &lt; best_aic: best_model=model best_aic=aic best_param=param results.append([param,model.aic])#输出最优模型result_table=pd.DataFrame(results)result_table.columns=['parameters','aic']print("最优模型：\n",best_model.summary()) 参数错误： (0, 2) 最优模型： ARMA Model Results ============================================================================== Dep. Variable: Weighted_Price No. Observations: 83 Model: ARMA(1, 1) Log Likelihood -688.761 Method: css-mle S.D. of innovations 957.764 Date: Mon, 15 Apr 2019 AIC 1385.522 Time: 14:42:56 BIC 1395.198 Sample: 12-31-2011 HQIC 1389.409 - 10-31-2018 ======================================================================================== coef std err z P&gt;|z| [0.025 0.975] ---------------------------------------------------------------------------------------- const 2103.6096 1567.542 1.342 0.183 -968.717 5175.936 ar.L1.Weighted_Price 0.9251 0.042 22.042 0.000 0.843 1.007 ma.L1.Weighted_Price 0.2681 0.116 2.311 0.023 0.041 0.495 Roots ============================================================================= Real Imaginary Modulus Frequency ----------------------------------------------------------------------------- AR.1 1.0809 +0.0000j 1.0809 0.0000 MA.1 -3.7302 +0.0000j 3.7302 0.5000 ----------------------------------------------------------------------------- 1234567#比特币预测df_month2=df_month[['Weighted_Price']]date_list=[datetime(2018,11,30),datetime(2018,12,31),datetime(2019,1,31),datetime(2019,2,28),datetime(2019,3,31), datetime(2019,4,30),datetime(2019,5,31),datetime(2019,6,30)]future=pd.DataFrame(index=date_list,columns=df_month.columns)df_month2=pd.concat([df_month2,future])df_month2['forecast']=best_model.predict(start=0,end=91) 123456789#比特币预测结果显示plt.figure(figsize=[20,7])df_month2.Weighted_Price.plot(label='实际金额')df_month2.forecast.plot(color='r',ls='--',label='预测金额')plt.legend()plt.title('比特币金额（月）')plt.xlabel('时间')plt.ylabel('美金')plt.show() 根据结果可以看出，8个月后的时间内，比特币的价格会触底到4000美金左右，而比特币的实际价格确实降低到了4000美金一以下。]]></content>
  </entry>
  <entry>
    <title><![CDATA[XGBoost]]></title>
    <url>%2Fce8cbd0b.html</url>
    <content type="text"><![CDATA[XGBoost]]></content>
  </entry>
  <entry>
    <title><![CDATA[信用卡欺诈分析]]></title>
    <url>%2F3e154dd7.html</url>
    <content type="text"><![CDATA[摘要针对欧洲某银行信用卡交易数据，构建一个信用卡交易欺诈识别器。采用逻辑回归算法，通过数据可视化方式对混淆矩阵进行展示，统计模型的精确率，召回率和F1值，以及ROC曲线与AUC的值。绘制精确率和召回率的曲线关系。 项目数据来源： https:/www.kaggle.com/mlg-ulb/creditcardfraud 项目背 数据集包含由欧洲持卡人于2013年9月使用信用卡进行交的数据。此数据集显示两天内发生的交易，其中284,807笔交易中有492笔被盗刷。数据集非常不平衡，积极的类（被盗刷）占所有交易的0.172％。它只包含作为PCA转换结果的数字输入变量。不幸的是，由于保密问题，我们无法提供有关数据的原始功能和更多背景信息。特征V1，V2，… V28是使用PCA获得的主要组件，没有用PCA转换的唯一特征是“时间”和“量”。特征’时间’包含数据集中每个事务和第一个事务之间经过的秒数。特征“金额”是交易金额，此特征可用于实例依赖的成本认知学习。特征’类’是响应变量，如果发生被盗刷，则取值1，否则为0。 算法选择 首先，我们拿到的数据是持卡人两天内的信用卡交易数据，这份数据包含很多维度，要解决的问题是预测持卡人是否会发生信用卡被盗刷。信用卡持卡人是否会发生被盗刷只有两种可能，发生被盗刷或不发生被盗刷。又因为这份数据是打标好的（字段Class是目标列），也就是说它是一个监督学习的场景。于是，我们判定信用卡持卡人是否会发生被盗刷是一个二元分类问题，意味着可以通过二分类相关的算法来找到具体的解决办法，本项目选用的算法是逻辑斯蒂回归（Logistic Regression）。 分析数据：数据是结构化数据 ，不需要做特征抽象。特征V1至V28是经过PCA处理，而特征Amount的数据规格与其他特征差别较大，需要对其做特征缩放，将特征缩放至同一个规格。在数据质量方面 ，没有出现乱码或空字符的数据，可以确定字段Class为目标列，其他列为特征列。 这份数据是全部打标好的数据，70%的数据进行训练，30%的数据进行预测和评估。 数据预处理12#数据加载df=pd.read_csv('./creditcard.csv') 1print(df.info()) &lt;class &apos;pandas.core.frame.DataFrame&apos;&gt; RangeIndex: 284807 entries, 0 to 284806 Data columns (total 31 columns): Time 284807 non-null float64 V1 284807 non-null float64 V2 284807 non-null float64 V3 284807 non-null float64 V4 284807 non-null float64 V5 284807 non-null float64 V6 284807 non-null float64 V7 284807 non-null float64 V8 284807 non-null float64 V9 284807 non-null float64 V10 284807 non-null float64 V11 284807 non-null float64 V12 284807 non-null float64 V13 284807 non-null float64 V14 284807 non-null float64 V15 284807 non-null float64 V16 284807 non-null float64 V17 284807 non-null float64 V18 284807 non-null float64 V19 284807 non-null float64 V20 284807 non-null float64 V21 284807 non-null float64 V22 284807 non-null float64 V23 284807 non-null float64 V24 284807 non-null float64 V25 284807 non-null float64 V26 284807 non-null float64 V27 284807 non-null float64 V28 284807 non-null float64 Amount 284807 non-null float64 Class 284807 non-null int64 dtypes: float64(30), int64(1) memory usage: 67.4 MB None 1print(df.head()) Time V1 V2 V3 V4 V5 V6 V7 \ 0 0.0 -1.359807 -0.072781 2.536347 1.378155 -0.338321 0.462388 0.239599 1 0.0 1.191857 0.266151 0.166480 0.448154 0.060018 -0.082361 -0.078803 2 1.0 -1.358354 -1.340163 1.773209 0.379780 -0.503198 1.800499 0.791461 3 1.0 -0.966272 -0.185226 1.792993 -0.863291 -0.010309 1.247203 0.237609 4 2.0 -1.158233 0.877737 1.548718 0.403034 -0.407193 0.095921 0.592941 V8 V9 ... V21 V22 V23 V24 \ 0 0.098698 0.363787 ... -0.018307 0.277838 -0.110474 0.066928 1 0.085102 -0.255425 ... -0.225775 -0.638672 0.101288 -0.339846 2 0.247676 -1.514654 ... 0.247998 0.771679 0.909412 -0.689281 3 0.377436 -1.387024 ... -0.108300 0.005274 -0.190321 -1.175575 4 -0.270533 0.817739 ... -0.009431 0.798278 -0.137458 0.141267 V25 V26 V27 V28 Amount Class 0 0.128539 -0.189115 0.133558 -0.021053 149.62 0 1 0.167170 0.125895 -0.008983 0.014724 2.69 0 2 -0.327642 -0.139097 -0.055353 -0.059752 378.66 0 3 0.647376 -0.221929 0.062723 0.061458 123.50 0 4 -0.206010 0.502292 0.219422 0.215153 69.99 0 [5 rows x 31 columns] 数据探索分析（EDA）12#数据探索print(df.describe()) Time V1 V2 V3 V4 \ count 284807.000000 2.848070e+05 2.848070e+05 2.848070e+05 2.848070e+05 mean 94813.859575 3.919560e-15 5.688174e-16 -8.769071e-15 2.782312e-15 std 47488.145955 1.958696e+00 1.651309e+00 1.516255e+00 1.415869e+00 min 0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00 25% 54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01 50% 84692.000000 1.810880e-02 6.548556e-02 1.798463e-01 -1.984653e-02 75% 139320.500000 1.315642e+00 8.037239e-01 1.027196e+00 7.433413e-01 max 172792.000000 2.454930e+00 2.205773e+01 9.382558e+00 1.687534e+01 V5 V6 V7 V8 V9 \ count 2.848070e+05 2.848070e+05 2.848070e+05 2.848070e+05 2.848070e+05 mean -1.552563e-15 2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15 std 1.380247e+00 1.332271e+00 1.237094e+00 1.194353e+00 1.098632e+00 min -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01 25% -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01 50% -5.433583e-02 -2.741871e-01 4.010308e-02 2.235804e-02 -5.142873e-02 75% 6.119264e-01 3.985649e-01 5.704361e-01 3.273459e-01 5.971390e-01 max 3.480167e+01 7.330163e+01 1.205895e+02 2.000721e+01 1.559499e+01 ... V21 V22 V23 V24 \ count ... 2.848070e+05 2.848070e+05 2.848070e+05 2.848070e+05 mean ... 1.537294e-16 7.959909e-16 5.367590e-16 4.458112e-15 std ... 7.345240e-01 7.257016e-01 6.244603e-01 6.056471e-01 min ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00 25% ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01 50% ... -2.945017e-02 6.781943e-03 -1.119293e-02 4.097606e-02 75% ... 1.863772e-01 5.285536e-01 1.476421e-01 4.395266e-01 max ... 2.720284e+01 1.050309e+01 2.252841e+01 4.584549e+00 V25 V26 V27 V28 Amount \ count 2.848070e+05 2.848070e+05 2.848070e+05 2.848070e+05 284807.000000 mean 1.453003e-15 1.699104e-15 -3.660161e-16 -1.206049e-16 88.349619 std 5.212781e-01 4.822270e-01 4.036325e-01 3.300833e-01 250.120109 min -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01 0.000000 25% -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02 5.600000 50% 1.659350e-02 -5.213911e-02 1.342146e-03 1.124383e-02 22.000000 75% 3.507156e-01 2.409522e-01 9.104512e-02 7.827995e-02 77.165000 max 7.519589e+00 3.517346e+00 3.161220e+01 3.384781e+01 25691.160000 Class count 284807.000000 mean 0.001727 std 0.041527 min 0.000000 25% 0.000000 50% 0.000000 75% 0.000000 max 1.000000 [8 rows x 31 columns] 123456plt.rcParams['font.sans-serif']=['SimHei'] #正确显示中文#绘制类别分布plt.figure()ax=sns.countplot(x='Class',data=df)plt.title('类别分布')plt.show() 123456#显示交易笔数，欺诈交易笔数num=len(df)num_fraud=len(df[df.Class==1])print('交易笔数：',num)print('欺诈交易笔数：',num_fraud)print('欺诈交易比例：&#123;:.4f&#125;'.format(num_fraud/num)) 交易笔数： 284807 欺诈交易笔数： 492 欺诈交易比例：0.0017 123456789#欺诈和正常交易可视化f,(ax1,ax2)=plt.subplots(2,1,sharex=True,figsize=(15,8))ax1.hist(df.Time[df.Class==1],bins=50,color='deeppink')ax1.set_title('诈骗交易')ax2.hist(df.Time[df.Class==0],bins=50,color='deepskyblue')ax2.set_title('正常交易')plt.xlabel('时间')plt.ylabel('交易次数')plt.show() 由欺诈交易比例显然可以看出这是一个不平衡数据集，正常交易笔数远大于欺诈交易笔数。对于不平衡数据集而言，如果分类结果全部归为正常交易，那么分类准确率仍然能达到99%。虽然准确率很高，显然这样的模型没什么用处，泛化能力低，根本无法判断出哪些是欺诈交易。因此需要在训练分类器之前对不平衡数据集进行处理。 重新平衡数据集 欠采样：从样本较多的类中抽取，仅保留这些样本点的一部分； 过采样：复制少数类中的一些点，以增加其基数； 生成合成数据：从少数类创建新的合成点，以增加其基数。 更改性能指标 混淆矩阵、Precision(查准率)、Recall(查全率)、F1score ROC曲线及AUC值 本文利用imblearn中的过采样方法SMOTE重新平衡数据集。 特征工程 由于amount特征中的数据与其他特征数据量级差距过大，对该特征数据进行标准化处理。 采用tran_test_split()对训练集与测试集按7，3成比例进行划分。 为保证测试结果的合理性，对训练集数据采用SMOTE进行过采样处理。 12345678910111213141516#对amount进行数据规范化df['Amount_Norm']=StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))#特征选择y=np.array(df.Class.tolist())df=df.drop(['Time','Amount','Class'],axis=1)X=df.as_matrix()#划分训练集与测试集train_x,test_x,train_y,test_y=train_test_split(X,y,test_size=0.1,random_state=33)#处理不平衡数据from imblearn.over_sampling import SMOTEsm=SMOTE(random_state=40)train_x,train_y=sm.fit_sample(train_x,train_y)n_sample=train_y.shape[0]n_neg_sample=train_y[train_y==0].shape[0]n_pos_sample=train_y[train_y==1].shape[0]print('样本个数：&#123;&#125;;负样本率&#123;:.2%&#125;;正样本率&#123;:.2%&#125;'.format(n_sample,n_pos_sample/n_sample,n_pos_sample/n_sample)) 样本个数：511788;负样本率50.00%;正样本率50.00% 构建模型构建分类器123456#逻辑回归分类clf=LogisticRegression()clf.fit(train_x,train_y)predict_y=clf.predict(test_x)#预测样本的置信分数score_y=clf.decision_function(test_x) 结果显示12345678910111213141516171819202122232425262728293031323334353637383940414243#定义功能函数#混淆矩阵可视化def plot_confusion_matrix(cm,classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues): plt.figure() plt.imshow(cm,interpolation='nearest',cmap=cmap) plt.title(title) plt.colorbar() tick_marks=np.arange(len(classes)) plt.xticks(tick_marks,classes,rotation=0) plt.yticks(tick_marks,classes) thresh=cm.max()/2.0 for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])): plt.text(j,i,cm[i,j], horizontalalignment='center', color='white' if cm[i,j]&gt;thresh else 'black') plt.tight_layout() plt.ylabel('True label') plt.xlabel('Predicted label') plt.show() #显示模型结果def show_metrics(): tp=cm[1,1] fn=cm[1,0] fp=cm[0,1] tn=cm[0,0] print('精确率：&#123;:.4f&#125;'.format(tp/(tp+fp))) print('召回率：&#123;:.4f&#125;'.format(tp/(tp+fn))) print('F1值：&#123;:.4f&#125;'.format(2*(tp/(tp+fp))*(tp/(tp+fn))/((tp/(tp+fp))+(tp/(tp+fn)))))#绘制精确率-召回率曲线def plot_precision_recall(): plt.step(recall,precision,color='b',alpha=0.2,where='post') plt.fill_between(recall,precision,step='post',alpha=0.2,color='b') plt.plot(recall,precision,linewidth=2) plt.xlim([0.0,1.0]) plt.ylim([0.0,1.05]) plt.xlabel('召回率') plt.ylabel('精确率') plt.title('精确率-召回率 曲线') plt.show() 12345#计算混淆矩阵cm=confusion_matrix(test_y,predict_y)class_names=[0,1]#显示混淆矩阵plot_confusion_matrix(cm,classes=class_names,title='逻辑回归模型的混淆矩阵') 12345#显示模型评估分数show_metrics()#计算精确率，召回率并可视化precision,recall,thresholds=precision_recall_curve(test_y,score_y)plot_precision_recall() 精确率：0.8409 召回率：0.6167 F1值：0.7115 12345678910111213141516from sklearn.metrics import roc_curve ,aucy_pred1_prob = clf.predict_proba(test_x)[:, 1] # 阈值默认值为0.5fpr, tpr, thresholds = roc_curve(test_y,y_pred1_prob)roc_auc = auc(fpr,tpr)# 绘制 ROC曲线plt.title('ROC Curve')plt.plot(fpr, tpr, 'b',label='AUC = %0.4f'% roc_auc)plt.legend(loc='lower right')plt.plot([0,1],[0,1],'r--')plt.xlim([-0.1,1.0])plt.ylim([-0.1,1.01])plt.ylabel('True Positive Rate')plt.xlabel('False Positive Rate')plt.show() 最终AUC的值达到0.9749，说明分类器的效果较好。]]></content>
  </entry>
  <entry>
    <title><![CDATA[数据分析流程及代码实现笔记]]></title>
    <url>%2Fa1ddee89.html</url>
    <content type="text"><![CDATA[将数据拆分成特征和标签部分 特征部分：模型运行所需要的特征数据 标签部分：目标列（字段） 方法1： 12X = data.ix[:, data.columns != 'Class']y = data.ix[:, data.columns == 'Class'] 方法2： 训练集-测试集一般分割方式一般分割方式：1234X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.4, random_state=0) clf_svc = svm.SVC(kernel='linear').fit(X_train,y_train)clf_svc.score(X_test,y_test) 优点： 简单、便捷 缺点： 浪费数据 容易过拟合,且矫正方式不方便 交叉验证：12345from sklearn.model_selection import cross_val_scoreclf_svc_cv = svm.SVC(kernel=&apos;linear&apos;,C=1)scores_clf_svc_cv = cross_val_score(clf_svc_cv,iris.data,iris.target,cv=5)print(scores_clf_svc_cv)print(&quot;Accuracy: %0.2f (+/- %0.2f)&quot; % (scores_clf_svc_cv.mean(), scores_clf_svc_cv.std() * 2)) 1234567from sklearn.model_selection import cross_validatefrom sklearn import metricsscoring = ['precision_macro', 'recall_macro']clf_cvs = svm.SVC(kernel='linear', C=1, random_state=0)scores_cvs = cross_validate(clf_cvs,iris.data,iris.target,cv=5,scoring=scoring,return_train_score = False)sorted(scores_cvs.keys()) 为了防止过拟合，通常的做法是将数据集分成训练集（train set）和测试集（test set）。 假设我们现在有个既定的模型。简单来说就拿LinearRegression() 来说。我们有一系列的可选超参。我们希望找到最优的一组超参。按照常规，我们的做法是先在train set 中训练模型，然后拿test set 去测试，找到$R^2$ 值最好的那个模型。 但是这样跳出来的模型，真的就是最好的吗? 不见得，这样得到的结果很大程度上取决于train set 和 test set的划分，也就是说，这样的评估有失公允。 换一个角度说，你在按照test set的角度挑选最优模型，很可能你挑出的模型在测试集上的效果ok但是在新的数据集上就没那么ok了（行话叫做鲁棒性（robust）不强）。 目前普遍的做法，是使用cross_validation来评估模型以及挑选模型。划重点： 在train set的人为的划分N折（默认是sklearn中默认3折）拿N-1折数据训练模型，剩下1折去验证模型，将N次实验的平均值作为评估结果。 然后再用test set去验证。 综合评估前后两次的结果 cv_results = cross_validate(estimator, X, y=None, groups=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’, return_train_score=’warn’) estimator： 必须是含有fit method的对象，可以是自己定义的Pipline X: 参与cross_validate 的featrues y: 默认为None，训练集标签(在监督学习的情况下) groups: array, shape(n_samples) ，这个用法很神奇，比如说你的train set 有350个就像本例，那么当你设置[1,1,….2,2] 一共350个1和2的时候，在训练集中就相当于2折。1的为一组，2的为1组。 cv: int，需要划分成几折。分类的情况下是SratiediedKFold，其他情况下是KFold n_jobs： int CPUs使用数量 fit_params: dict, optional 传入estimator fit method的参数 ,pre_dispatch: int, 控制并行计算的jobs,减少数量，可以避免内存爆炸 return_train_score: 是否返回训练评估参数，默认是True 在0.21版本后会改为False,建议改为False减少计算量 类不平衡数据处理 K折交叉实验12from sklearn.model_selection import KFoldsklearn.model_selection.KFold(n_splits=3, shuffle=False, random_state=None) 思路：将训练/测试数据集划分n_splits个互斥子集，每次用其中一个子集当作验证集，剩下的n_splits-1个作为训练集，进行n_splits次训练和测试，得到n_splits个结果。注意点：对于不能均等份的数据集，其前n_samples % n_splits子集拥有n_samples // n_splits + 1个样本，其余子集都只有n_samples // n_splits样本。 参数说明： n_splits：表示划分几等份 shuffle：在每次划分时，是否进行洗牌 若为Falses时，其效果等同于random_state等于整数，每次划分的结果相同 若为True时，每次划分的结果都不一样，表示经过洗牌，随机取样的 random_state：随机种子数 属性： get_n_splits(X=None, y=None, groups=None)：获取参数n_splits的值 split(X, y=None, groups=None)：将数据集划分成训练集和测试集，返回索引生成器 设置shuffle=False，运行两次，发现两次结果相同 实例： 1234567891011121314151617181920212223242526272829In [3]: from sklearn.model_selection import KFold ...: import numpy as np ...: X = np.arange(24).reshape(12,2) ...: y = np.random.choice([1,2],12,p=[0.4,0.6]) ...: kf = KFold(n_splits=5,shuffle=True) ...: for train_index , test_index in kf.split(X): ...: print('train_index:%s , test_index: %s ' %(train_index,test_index)) ...: ...:train_index:[ 0 1 2 4 5 6 7 8 10] , test_index: [ 3 9 11]train_index:[ 0 1 2 3 4 5 9 10 11] , test_index: [6 7 8]train_index:[ 2 3 4 5 6 7 8 9 10 11] , test_index: [0 1]train_index:[ 0 1 3 4 5 6 7 8 9 11] , test_index: [ 2 10]train_index:[ 0 1 2 3 6 7 8 9 10 11] , test_index: [4 5] In [4]: from sklearn.model_selection import KFold ...: import numpy as np ...: X = np.arange(24).reshape(12,2) ...: y = np.random.choice([1,2],12,p=[0.4,0.6]) ...: kf = KFold(n_splits=5,shuffle=True) ...: for train_index , test_index in kf.split(X): ...: print('train_index:%s , test_index: %s ' %(train_index,test_index)) ...: ...:train_index:[ 0 1 2 3 4 5 7 8 11] , test_index: [ 6 9 10]train_index:[ 2 3 4 5 6 8 9 10 11] , test_index: [0 1 7]train_index:[ 0 1 3 5 6 7 8 9 10 11] , test_index: [2 4]train_index:[ 0 1 2 3 4 6 7 9 10 11] , test_index: [5 8]]]></content>
  </entry>
  <entry>
    <title><![CDATA[特征工程笔记]]></title>
    <url>%2F5174f740.html</url>
    <content type="text"><![CDATA[摘要笔记内容主要是在scikit-learn(sklearn)中使用特征工程的具体方法（代码实现）。 一、数据清洗 数据缺失 数据重复 数据异常 解决办法]]></content>
  </entry>
  <entry>
    <title><![CDATA[信用卡违约率分析]]></title>
    <url>%2F7a8b0d76.html</url>
    <content type="text"><![CDATA[摘要针对台湾某银行信用卡的数据，采用scikit-learn中的pipeline机制创建5个分类器并寻找最优参数，选择准确率最高的CART分类器，信用卡违约率识别率在81.9%左右。 背景首先理解业务，什么是信用卡违约？信用卡违约行为一般是指持卡人未按约定在到期还款日（含）前偿还最低还款额。为什么要分析信用卡违约率？银行需要把控风控标准来降低信用卡债危机趋势。本文信用卡违约率分析即针对银行信用卡的数据集，构建一个分析信用卡违约率的分类器，预测用户是否会发生违约行为。 数据获取数据集信息本项目数据集是台湾某银行2005年4月到9月的信用卡数据，来源于kaggle。数据来源：https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset/version/1 理解数据集中的字段含义数据集一共包括25个字段，具体含义如下： 字段 含义 ID 客户ID LIMIT_BAL 可透支金额 SEX 性别（1 =男性，2 =女性） EDUCATION 教育（1 =研究生院，2 =大学，3 =高中，4 =其他，5 =未知，6 =未知） MARRIAGE 婚姻状况（1 =已婚，2 =单身，3 =其他） AGE 年龄 PAY_0 2005年9月的还款状况（-1 =正常支付，1 =一个月的付款延迟，2 =两个月的付款延迟，… 8 = 8个月的付款延迟，9 = 9个月的付款延迟和以上） PAY_2 2005年8月的还款状况（规模与上述相同） PAY_3 2005年7月的还款状况（规模与上述相同） PAY_4 2005年6月的还款状况（规模与上述相同） PAY_5 2005年5月的还款状况（规模与上述相同） PAY_6 2005年4月的还款状况（规模与上述相同） BILL_AMT1 2005年9月的账单金额 BILL_AMT2 2005年8月的账单金额 BILL_AMT3 2005年7月的账单金额 BILL_AMT4 2005年6月的账单金额 BILL_AMT5 2005年5月的账单金额 BILL_AMT6 2005年4月的账单金额 PAY_AMT1 2005年9月的还款金额 PAY_AMT2 2005年8月的还款金额 PAY_AMT3 2005年7月的还款金额 PAY_AMT4 2005年6月的还款金额 PAY_AMT5 2005年5月的还款金额 default.payment.next.month 下个月是否违约（1=违约，0=守约） 数据加载导入包1234567891011121314151617#主题：信用卡违约率分析#导入各类包import pandas as pdimport sklearnfrom sklearn.model_selection import learning_curve,train_test_split,GridSearchCVfrom sklearn.preprocessing import StandardScalerfrom sklearn.pipeline import Pipelinefrom sklearn.metrics import accuracy_scorefrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import RandomForestClassifierfrom sklearn.neighbors import KNeighborsClassifierfrom matplotlib import pyplot as plt%matplotlib inlineimport seaborn as snsimport sysfrom os import path 数据加载12#数据加载data = pd.read_csv('./UCI_Credit_Card.csv') 数据准备数据探索123print(data.shape) #查看数据集大小print(data.info()) #查看数据信息print(data.describe()) #数据集描述 结果(30000, 25)数据集共有25个字段，30000条记录 数据集中并无缺失值，且数据类型正常。 查看下一个月违约率的情况12345678910#查看下一个月违约率的情况next_month=data['default.payment.next.month'].value_counts()df=pd.DataFrame(&#123;'default.payment.next.month': next_month.index,'values':next_month.values&#125;)plt.rcParams['font.sans-serif']=['SimHei'] #显示中文标签plt.figure(figsize=(6,6))plt.title('信用卡违约率客户\n (违约：1，守约：0)')sns.set_color_codes("pastel")sns.barplot(x='default.payment.next.month',y='values',data=df)locs,labels=plt.xticks()plt.show() 特征处理ID字段对目标结果无影响，去除该字段；default.payment.next.month作为判断结果的字段。因为数据集中没有专门的测试集，所以使用 train_test_split将数据集中的30%作为测试集，其余作为训练集。12345678#特征处理data.drop(['ID'],inplace=True,axis=1) #ID字段对目标结果无影响，去除该字段target=data['default.payment.next.month'].valuescolumns=data.columns.tolist()columns.remove('default.payment.next.month') #去除结果字段features=data[columns].values #保留的特征#将数据集中的30%作为测试集，其余作为训练集train_x,test_x,train_y,test_y=train_test_split(features,target,test_size=0.3,stratify=target,random_state=1) 分类阶段在该阶段把构建模型于模型评估部分放在一起。因为我们可以用Pipeline管道机制，将数据规范化设置为第一步，分类为第二步。目前我们不知道采用哪个分类器效果更好，因此，同时采用了SVM（支持向量机）、决策树、Random Forest（随机森林）和KNN（最近邻）。然后通过GridSearchCV工具，找到每个分类器的最优参数和最优分数，最终找到适合这个项目的分类器和该分类器的参数。 123456789101112131415161718192021222324252627282930313233343536373839#构造分类器classifiers=[ SVC(random_state=1,kernel='rbf'), DecisionTreeClassifier(random_state=1,criterion='gini'), RandomForestClassifier(random_state=1,criterion='gini'), KNeighborsClassifier(metric='minkowski') AdaBoostClassifier(random_state=1)]#the names of classifiersclassifier_names=['svc','decisionTreeClassifier','randomForestClassifier','kneighborsClassifier','adaBoostClassifier']#the attribution of classifierclassifier_param_grid=[ &#123;'svc__C':[1],'svc__gamma':[0.01]&#125;, &#123;'decisionTreeClassifier__max_depth':[4,9,11]&#125;, &#123;'randomForestClassifier__n_estimators':[3,5,8]&#125;, &#123;'kneighborsClassifier__n_neighbors':[4,6,8]&#125;, &#123;'adaBoostClassifier__n_estimators':[10,50,100]&#125;]#利用GridSearchCV对分类器进行参数调优def GridSearchCV_work(pipeline,train_x,train_y,test_x,test_y,param_grid,score='accuracy'): response=&#123;&#125; gridsearch=GridSearchCV(estimator=pipeline,param_grid=param_grid,scoring=score) #寻找最优的参数和最优的准确率分数 search=gridsearch.fit(train_x,train_y) print("GridSearch最优参数：",search.best_params_) print("GridSearch最优分数：%0.4lf" %search.best_score_) predict_y=gridsearch.predict(test_x) print("准确率 %0.4lf" %accuracy_score(test_y,predict_y)) response['predict_y']=predict_y response['accuracy_score']=accuracy_score(test_y,predict_y) return responsefor model,model_name,model_param_grid in zip(classifiers,classifier_names,classfier_param_grid): pipeline=Pipeline([ ('scaler',StandardScaler()), (model_name,model) ]) result=GridSearchCV_work(pipeline,train_x,train_y,test_x,test_y,model_param_grid,score='accuracy') 得到计算结果GridSearch最优参数： {‘svc__C’: 1, ‘svc__gamma’: 0.01}GridSearch最优分数：0.8186准确率 0.8172GridSearch最优参数： {‘decisionTreeClassifier__max_depth’: 4}GridSearch最优分数：0.8208准确率 0.8191GridSearch最优参数： {‘randomForestClassifier__n_estimators’: 8}GridSearch最优分数：0.8024准确率 0.8036GridSearch最优参数： {‘kneighborsClassifier__n_neighbors’: 8}GridSearch最优分数：0.8040准确率 0.8036GridSearch最优参数： {‘adaBoostClassifier__n_estimators’: 10}GridSearch最优分数：0.8188准确率 0.8129]]></content>
  </entry>
  <entry>
    <title><![CDATA[测试titanic项目]]></title>
    <url>%2F280f2cc6.html</url>
    <content type="text"><![CDATA[泰坦尼克号乘客生存预测案例背景泰坦尼克号沉船事故是世界上最著名的沉船事故之一。1912年4月15日，在她的处女航期间，泰坦尼克号撞上冰山后沉没，造成2224名乘客和机组人员中超过1502人的死亡。这一轰动的悲剧震惊了国际社会，并导致更好的船舶安全法规。 事故中导致死亡的一个原因是许多船员和乘客没有足够的救生艇。然而在被获救群体中也有一些比较幸运的因素；一些人群在事故中被救的几率高于其他人，比如妇女、儿童和上层阶级。 这个Case里，我们需要分析和判断出什么样的人更容易获救。最重要的是，要利用机器学习来预测出在这场灾难中哪些人会最终获救； 数据挖掘流程(一)数据读取 读取数据，并进行展示 统计数据各项指标 明确数据规模与要完成的任务 (二)特征理解分析 单特征分析，逐个变量分析其对结果的影响 多变量统计分析，综合考虑多种情况影响 统计绘图得出结论 (三)数据清洗与预处理 对缺失值进行填充 特征标准化/归一化 筛选有价值的特征 分析特征之间的相关性 (四)建立模型 特征数据与标签准备 数据集切分 多种建模算法对比 集成策略等方案改进 数据样本此项目数据集分为2份数据集：titanic_train.csv和titanic_test.csv titanic_train.csv: 训练集，共计891条数据titanic_test.csv: 测试集，共计418条数据 数据特征 字段 字段说明 PassengerId 乘客编号 Survived 存活情况(存活:1; 死亡:0) Pclass 客舱等级 Name 乘客姓名 Sex 性别 Age 年龄 SibSp 同乘的兄弟姐妹/配偶数 Parch 同乘的父母/小孩数 Ticket 船票编号 Fare 船票价格 Cabin 客舱号 Embarked 登船港口 PassengerId是数据唯一序号；Survived是存活情况，为预测标记特征；剩下的10个是原始特征数据。 数据统计与统计分析 查看哪些列有缺失值？ 属性 数目 PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 从上面可以看出来Cabin这个属性缺失值比较多 整体看看数据啥规模？(利用Pandas的describe()函数) PassengerId Survived Pclass Age SibSp Parch Fare count 891.000000 891.000000 891.000000 714.000000 891.000000 891.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 0.381594 32.204208 std 257.353842 0.486592 0.836071 14.526497 1.102743 0.806057 49.693429 min 1.000000 0.000000 1.000000 0.420000 0.000000 0.000000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 0.000000 7.910400 50% 446.000000 0.000000 3.000000 28.000000 0.000000 0.000000 14.454200 75% 668.500000 1.000000 3.000000 38.000000 1.000000 0.000000 31.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 6.000000 512.329200 mean字段告诉我们，大概0.383838的人最后获救了，2/3等舱的人数比1等舱要多，平均乘客年龄大概是29.7岁(计算这个时候会略掉无记录的)等等… 通过绘图来看看获救比例咋样 性别特征分析数据特征分为：连续值和离散值 离散值：性别(男、女)，登船地点(S, Q, C) 连续值：年龄，船票价格 按照性别进行分组分别输出获救人数： Sex Survived Sum female 0 81 female 1 233 male 0 468 male 1 109 画出相应的图如下所示： 船舱等级特征分析 Survivied 0 1 All Pclass 1 80 136 216 2 97 87 184 3 372 119 491 All 549 342 891 我们可以清楚地看到，船舱等级为1的被给予很高的优先级而救援。尽管数量在Pclass3乘客高了很多，仍然存活数从他们是非常低的，大约25% 对于Pclass1来说存活是63%左右，而Pclass2大约是48%。所以金钱和地位很重要。这样一个物欲横流的世界。 那这些又和性别有关吗？接下来我们再来看看船舱等级和性别对结果的影响。(多变量分析) 我们可以很容易地推断，从Pclass1女性生存是95-96%，如94人中只有3位女性从Pclass1没获救。 显而易见的是，不论Pclass，女性优先考虑。 看来Pclass也是一个重要的特征。让我们分析其他特征。 年龄特征缺失值填充与分析(连续值) Oldest Passenger was of: 80.0 Years Youngest Passenger was of: 0.42 Years Average Age on the ship: 29.69911764705882 Years 可以得出以下结论： 10岁以下儿童的存活率随passenegers数量增加； 生存为20-50岁获救几率更高一些； 对男性来说，随着年龄的增长，存活率降低 缺失值填充 平均值 经验值 回归模型预测 剔除掉 正如我们前面看到的，年龄特征有177个空值。为了替换这些缺失值，我们可以给它们分配数据集的平均年龄。 但问题是，有许多不同年龄的人。最好的办法是找到一个合适的年龄段！ 我们可以检查名字特征。根据这个特征，我们可以看到名字有像先生或夫人这样的称呼，这样我们就可以把先生和夫人的平均值分配给各自的组。 观察： 幼儿(年龄在5岁以下(获救的还是蛮多的(妇女和儿童优先政策) 最老的乘客得救了(80年) 死亡人数最高的是30-40岁年龄组 登船地点特征分析 C港生存的可能性最高在0.55左右，而S的生存率最低。 观察: 大部分人的船舱等级是3 C的乘客看起来很幸运，他们中的一部分幸存下来 S港口的富人蛮多的。仍然生存的机会很低 港口Q几乎有95%的乘客都是穷人 观察: 存活的几率几乎为1, 在Pclass1和Pclass2中的女人 Pclass3的乘客中男性和女性的生存率都是很偏低的 港口Q很不幸，因为那里都是3等舱的乘客 Note: 港口中也存在缺失值，在这里用众数来进行填充了，因为S登船人最多呀 家庭特征分析兄弟姐妹的数量 这个特征表示一个人是独自一人还是与他的家人在一起 Survived 0 1 SibSp 0 398 210 1 97 112 2 15 13 3 12 4 4 15 3 5 5 0 8 7 0 Pclass 1 2 3 SibSp 0 137 120 351 1 71 55 83 2 5 8 15 3 3 1 12 4 0 0 18 5 0 0 5 8 0 0 7 观察： 如果乘客是孤独的船上没有兄弟姐妹，他有34.5%的存活率。如果兄弟姐妹的数量增加，概率大致减少。这是有道理的。也就是说，如果我有一个家庭在船上，我会尽力拯救他们，而不是先救自己。但是令人惊讶的是，5-8名成员家庭的存活率为0%。原因可能是他们在pclass=3的船舱？ 父母和孩子的数量 Pclass 1 2 3 Parch 0 163 134 381 1 31 32 55 2 21 16 43 3 0 2 3 4 1 0 3 5 0 0 5 6 0 0 1 以上再次表明，大家庭都在Pclass3 观察： 这里的结果也很相似。带着父母的乘客有更大的生存机会。然而，它随着数字的增加而减少。 在船上的家庭父母人数中有1-3个的人的生存机会是好的。独自一人也证明是致命的，当船上有4个父母时，生存的机会就会减少。 船票的价格特征分析 Highest Fare was: 512.3292 Lowest Fare was: 0.0 Average Fare was: 32.204207968574636 特征相关性概括地观察所有的特征: 性别：与男性相比，女性的生存机会很高 Pclass：有第一类乘客给你更好的生存机会的一个明显趋势。对于Pclass3成活率很低。对于女性来说，从Pclass1生存的机会几乎是100% 年龄：小于5-10岁的儿童存活率高。年龄在15到35岁之间的乘客死亡很多 港口：上来的仓位也有区别，死亡率也很大！ 家庭：有1-2的兄弟姐妹、配偶或加上父母有1-3人, 而不是独自一人或有一个大家庭旅行，你有更大的概率存活下来。 首先要注意的是，只有数值特征进行比较 正相关：如果特征A的增加导致特征B的增加，那么它们呈正相关。值1表示完全正相关。 负相关：如果特征A的增加导致特征B的减少，则呈负相关。值-1表示完全负相关。 现在让我们说两个特性是高度或完全相关的，所以一个增加导致另一个增加。这意味着两个特征都包含高度相似的信息，并且信息很少或没有变化。这样的特征对我们来说是没有价值的！ 那么你认为我们应该同时使用它们吗？。在制作或训练模型时，我们应该尽量减少冗余特性，因为它减少了训练时间和许多优点。 现在，从上面的图，我们可以看到，特征不显著相关。 特征工程和数据清洗当我们得到一个具有特征的数据集时，是不是所有的特性都很重要？可能有许多冗余的特征应该被消除，我们还可以通过观察或从其他特征中提取信息来获得或添加新特性。 连续特征离散化(年龄) 年龄的取值范围是[0, 80], 我们将其分为5组。 Age_band|Sum||:-:|:-:||1|382||2|325||0|104||3|69||4|11| Note: 0表示0-16岁，1表示17-32岁，2表示33-48岁，3表示49-64岁，5表示65-80岁 从上图可以看出：生存率随年龄的增加而减少，不论Pclass。 Family_size：家庭总人数 光看兄弟姐妹和老人孩子看感觉不太直接，这里我们直接看全家的人数 从上图可以看出：Family_size = 0意味着passeneger是孤独的。显然，如果你是单独或family_size = 0，那么生存的机会很低。家庭规模4以上，机会也减少。这看起来也是模型的一个重要特性。让我们进一步研究这个问题 连续特征离散化(船票价格) 因为票价也是连续的特性，所以我们需要将它离散化 Survived Fare_Range (-0.001, 7.91] 0.197309 (7.91, 14.454] 0.303571 (14.454, 31.0] 0.454955 (31.0, 512.329] 0.581081 如上所述，我们可以清楚地看到，船票价格增加生存的机会增加. 显然，随着fare_cat增加，存活的几率增加。随着性别的变化，这一特性可能成为建模过程中的一个重要特征。 总结： 去掉一些不必要的特征： 乘客姓名 —&gt; 我们不需要name特性，因为它不能转换成任何分类值 年龄 —&gt; 我们有age_band特征，所以不需要这个年龄特征 船票编号 —&gt; 这是任意的字符串，不能被归类 船票价格 —&gt; 我们有fare_cat特征，所以不需要船票价格特征 客仓号 —&gt; 这个也不需要，因为没啥含义 乘客编号 —&gt; 不能被归类 我们再来看看去掉部分无用特征后的特征之间的关系图： 现在我们再看以上的相关图，我们可以看到一些正相关的特征。 机器学习建模我们从EDA部分获得了一些见解。但是，我们不能准确地预测或判断一个乘客是否会幸存或死亡。现在我们将使用一些很好的分类算法来预测乘客是否能生存下来： Logistic回归 支持向量机(线性和径向) 随机森林 k-近邻 朴素贝叶斯 决策树 神经网络 以下是这些模型单个的预测结果： 无特殊声明，均使用默认参数 Radial Support Vector Machines(rbf-SVM) Accuracy for rbf SVM is 0.835820895522388 Linear Support Vector Machine(linear-SVM) Accuracy for linear SVM is 0.8171641791044776 Logistic Regression The accuracy of the Logistic Regression is 0.8171641791044776 Decision Tree The accuracy of the Decision Tree is 0.7985074626865671 K-Nearest Neighbours(KNN) The accuracy of the KNN is 0.832089552238806 Naive Bayes The accuracy of the NaiveBayes is 0.8134328358208955 Random Forest The accuracy of the Random Forests is 0.8097014925373134 模型的精度并不是决定分类器效果的唯一因素。假设分类器在训练数据上进行训练，需要在测试集上进行测试才有效果 现在这个分类器的精确度很高，但是我们可以确认所有的新测试集都是90%吗？答案是否定的，因为我们不能确定分类器在不同数据源上的结果。当训练和测试数据发生变化时，精确度也会改变。它可能会增加或减少 为了克服这一点，得到一个广义模型，我们使用交叉验证 交叉验证一个测试集看起来不太够呀，多轮求均值是一个好的策略. 交叉验证的工作原理是首先将数据集分成k-subsets(K个大小相似的互斥子集) 假设我们将数据集划分为（k＝5）部分。我们预留1个部分进行测试，并对其他4个部分进行训练 我们通过在每次迭代中改变测试部分并在其他部分中训练算法来继续这个过程。然后对衡量结果求平均值，得到算法的平均精度 以上就是所谓的交叉验证 Machine Learning Methods CV Mean Std Linear Svm 0.793471 0.047797 Radial Svm 0.828290 0.034427 Logistic Regression 0.805843 0.021861 KNN 0.813783 0.041210 Decision Tree 0.804757 0.029890 Naive Bayes 0.801386 0.028999 Random Forest 0.815968 0.035414 解释混淆矩阵：来看第一个图 预测的正确率为491(死亡) + 247(存活)，平均CV准确率为(491+247)/ 891＝82.8% 58和95都是我们的算法预测错了的 超参数整定机器学习模型就像一个黑盒子。这个黑盒有一些默认参数值，我们可以调整或更改以获得更好的模型。比如支持向量机模型中的C和γ，我们称之为超参数，他们对结果可能产生非常大的影响 主要是利用网格搜索来选出最优参数 例如，进行网格搜索之后：RBF支持向量机的最佳得分为82.82%，C＝0.5，γ＝0.1。RandomForest，成绩是81.8% 集成模块集成是提高模型的精度和性能的一个很好的方式。简单地说，是各种简单模型的结合创造了一个强大的模型 Bagging 类似随机森林类型的，并行的集成 Boosting 提升类型 Stacking 堆叠类型 Blending BaggingBagging将多个模型，也就是多个基学习器的预测结果进行简单的加权平均或者投票。它的好处是可以并行地训练基学习器。Random Forest就用到了Bagging的思想。 Bagging KNN The accuracy for bagged KNN is: 0.835820895522 The cross validated score for bagged KNN is: 0.814889342867 Bagging Decision Tree The accuracy for bagged Decision Tree is: 0.824626865672 The cross validated score for bagged Decision Tree is: 0.820482635342 BoostingBoosting的思想有点像知错能改，每个基学习器是在上一个基学习器学习的基础上，对上一个基学习器的错误进行弥补。我们将会用到的AdaBoost，Gradient Boost就用到了这种思想 提升是一个逐步增强的弱模型，首先对完整的数据集进行训练。现在模型会得到一些实例，而有些错误。现在，在下一次迭代中，学习者将更多地关注错误预测的实例或赋予它更多的权重. AdaBoost(自适应增强)—在这种情况下，弱学习或估计是一个决策树。但我们可以改变缺省base_estimator任何算法的选择 Adaboost The cross validated score for AdaBoost is: 0.8249526160481218 The cross validated score for Gradient Boosting is: 0.818286233118 StackingStacking是用新的次学习器去学习如何组合上一层的基学习器。如果把Bagging看作是多个基分类器的线性组合，那么Stacking就是多个基分类器的非线性组合。Stacking可以将学习器一层一层地堆砌起来，形成一个网状的结构 相比来说Stacking的融合框架相对前面的二者来说在精度上确实有一定的提升。 BlendingBlending和Stacking很相似，但同时它可以防止信息泄露的问题。 我们得到了最高的精度为AdaBoost。我们将尝试用超参数调整来增加它 我们可以从AdaBoost的最高精度是83.16%，n_estimators = 200和learning_rate = 0.05 Confusion Matrix for the Best Model 特征重要性衡量]]></content>
  </entry>
  <entry>
    <title><![CDATA[慎独]]></title>
    <url>%2F4a17b156.html</url>
    <content type="text"><![CDATA[写给自己的话 1.保持专注，培养成一种本能 2.昨天-今天-明天 不为昨天自责，过去终究是过去 过好今天的每一刻，当今天成为昨天时，没有遗憾 计划好明天，当明天成为今天时，让事情按它的应有的轨迹运行 3.天下事有难易乎？为之，难者亦易矣；不为，易者亦难矣。人之为学有难易乎？学之，则难者亦易矣；不学，则易者亦难矣。 4.上士闻道，勤而习之；中士闻道，若存若之；下士闻道，大笑之。 5.克制是一种美德 6.做事可以不连续，但是一定要有计划，一定要连续。一鼓作气，再而衰，三而竭适用于短期的工作。一项长期的计划应当使用闭环管理。]]></content>
  </entry>
</search>
